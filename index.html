<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>rizma</title>
  <style>
    :root { --container-w: min(760px, calc(100% - 48px)); }
    .container { width: var(--container-w); margin: 0 auto; box-sizing: border-box; }
    /* End (×) button: filled circle, matches mic size */
    #endSession {
      width: 44px; height: 44px; border-radius: 22px; /* match micFab */
      display: flex; align-items: center; justify-content: center;
      background: #F5F5F5; color: #0E0E10; /* match waveform gray */
      box-shadow: 0 2px 6px rgba(0,0,0,0.06);
      transition: background-color .15s ease, transform .04s ease;
    }
    #endSession:hover { background: #FBD5C7; }            /* peach on hover (same as mic) */
    #endSession:active { background: #F7C2B0; transform: translateY(0.5px); }
    #micFab { cursor: pointer; background: #F5F5F5; transition: background-color .15s ease; }
    #micFab:hover { background: #FBD5C7; }
  </style>
</head>

<body
  style="font-family:-apple-system, BlinkMacSystemFont, 'SF Pro Text', 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; background:#FFFFFF; color:#0E0E10; display:flex; flex-direction:column; align-items:center; gap:24px; min-height:100vh; margin:0;">
  <!-- Header / Brand -->
  <header style="margin-top:40px; display:flex; flex-direction:column; align-items:center; gap:8px;">
    <div style="display:flex; align-items:center; gap:10px;">
      <img src="rizma_logo_color.png" alt="rizma logo"
        style="width:36px; height:36px; box-shadow:0 1px 2px rgba(0,0,0,0.06);" />
      <div style="font-size:28px; font-weight:700; letter-spacing:0.2px;">rizma</div>
    </div>
    <div style="font-size:16px; color:#6B6B6B;">Support Between Sessions</div>
  </header>

  <!-- Therapist Card -->
  <section class="container"
    style="width:var(--container-w); background:#FFFFFF; border:1px solid #EEE; border-radius:20px; padding:32px; box-shadow:0 8px 24px rgba(0,0,0,0.05); display:flex; flex-direction:column; align-items:center; gap:16px;">
    <img src="rizma-doc-3-elena_cropped.png" alt="Elena"
      style="width:120px; height:120px; border-radius:60px; object-fit:cover; box-shadow:0 2px 6px rgba(0,0,0,0.06);" />
    <div style="font-size:28px; font-weight:700;">Elena</div>
    <div style="font-size:16px; color:#6B6B6B; text-align:center; max-width:520px;">Helps with homework and answers
      questions.</div>
    <button id="hold" aria-pressed="false" aria-label="Start Voice Session"
      style="font-size:18px; font-weight:600; padding:16px 24px; border-radius:14px; background:#FBD5C7; color:#0E0E10; border:none; width:100%; max-width:420px; box-shadow:0 2px 6px rgba(0,0,0,0.06); display:none;">
      <svg viewBox="0 0 32 32" width="28" height="28" aria-hidden="true" focusable="false" style="display:block;"
        fill="currentColor">
        <rect x="2" y="12" width="4" height="8" rx="2"></rect>
        <rect x="8" y="8" width="4" height="16" rx="2"></rect>
        <rect x="14" y="4" width="4" height="24" rx="2"></rect>
        <rect x="20" y="8" width="4" height="16" rx="2"></rect>
        <rect x="26" y="12" width="4" height="8" rx="2"></rect>
      </svg>
    </button>
  </section>

  <audio id="remoteAudio" autoplay playsinline style="display:none;"></audio>
  <!-- Fallback audio for HTTP TTS when Realtime fails -->
  <audio id="fallbackAudio" playsinline style="display:none;"></audio>

  <!-- Conversation Panel -->
  <section id="panel" class="container"
    style="width:var(--container-w); background:#FFFFFF; color:#0E0E10; border:1px solid #EEE; border-radius:16px; padding:12px 16px; box-shadow:0 4px 14px rgba(0,0,0,0.04); display:none;">
    <div style="display:flex; justify-content:flex-end;">
      <button id="endSession" aria-label="End session" title="End session"
        style="border:none; font-size:22px; line-height:1; padding:0; cursor:pointer; color:#0E0E10;">
        &times;
      </button>
    </div>
    <div id="status" style="font-size:12px; opacity:0.7; margin-bottom:8px;">Idle</div>
    <div id="chat"
      style="display:flex; flex-direction:column; gap:8px; max-height:40vh; overflow:auto; padding-right:6px;"></div>
  </section>

  <!-- Chat composer with waveform mic, button hidden above -->
  <section id="composer" class="container"
    style="width:var(--container-w); display:flex; align-items:center; gap:12px; margin-top:6px;">
    <button id="composerPlus" aria-label="Add" title="Add"
      style="width:36px; height:36px; border-radius:18px; background:#F5F5F5; border:none; color:#0E0E10; font-size:22px; line-height:36px; display:flex; align-items:center; justify-content:center;">
      +
    </button>
    <div id="composerInput"
      style="flex:1; height:44px; border-radius:22px; background:#F5F5F5; color:#8B8B8B; display:flex; align-items:center; padding:0 14px; font-size:16px;">
      Ask anything
    </div>
    <button id="micFab" aria-label="Start Voice Session" title="Start Voice Session"
      style="width:44px; height:44px; border:none; border-radius:22px; color:#0E0E10; display:flex; align-items:center; justify-content:center; box-shadow:0 2px 6px rgba(0,0,0,0.06);">
      <svg viewBox="0 0 32 32" width="22" height="22" aria-hidden="true" focusable="false" fill="currentColor">
        <rect x="2" y="12" width="4" height="8" rx="2"></rect>
        <rect x="8" y="8" width="4" height="16" rx="2"></rect>
        <rect x="14" y="4" width="4" height="24" rx="2"></rect>
        <rect x="20" y="8" width="4" height="16" rx="2"></rect>
        <rect x="26" y="12" width="4" height="8" rx="2"></rect>
      </svg>
    </button>
  </section>

  <!-- Footer -->
  <footer style="margin:8px 0 24px; color:#8B8B8B; font-size:14px; display:flex; gap:8px; align-items:center;">
    <span>Secure &bull; Private &bull; Professional</span>
    <button id="reset"
      style="margin-left:12px; font-size:12px; color:#6B6B6B; background:transparent; border:none; text-decoration:underline; cursor:pointer;">Reset
      Session</button>
  </footer>

  <script>
    // OpenAI key stays in Cloudflare Worker secret; browser calls proxy
    const API_BASE = "https://rizma-proxy.rizma.workers.dev/openai";

    let isRecording = false;
    let recStartedAt = 0; // kept for UI compatibility; not used by WebRTC path
    let hasSessionStarted = false; // tracks whether we've recorded at least once
    let chunks = []; // legacy placeholder; no longer used

    const btn = document.getElementById('hold');
    const statusEl = document.getElementById('status');
    const chatEl = document.getElementById('chat');
    const micFab = document.getElementById('micFab');
    const panelEl = document.getElementById('panel');
    const composerEl = document.getElementById('composer');
    const endBtn = document.getElementById('endSession');

    // Realtime (WebRTC) constants
    const REALTIME_MODEL = "gpt-realtime"; // per OpenAI Realtime GA; see docs
    const SESSION_URL = "https://rizma-proxy.rizma.workers.dev/session"; // absolute Worker endpoint (POST is supported here)
    const SERVER_VAD = true; // matches session.update turn_detection

    // Realtime state
    let pc = null;           // RTCPeerConnection
    let dc = null;           // RTCDataChannel for events
    let micStream = null;    // MediaStream from getUserMedia
    let isConnected = false; // WebRTC session status
    let isConnecting = false; // prevents duplicate offers / multiple /session calls

    // Buffers for streaming transcripts
    let assistantBuf = "";

    // --- Debug: remote audio & stats ---
    function attachRemoteAudioDebug(el) {
      if (!el) return;
      el.addEventListener('play', () => console.log('remoteAudio: play'));
      el.addEventListener('pause', () => console.log('remoteAudio: pause'));
      el.addEventListener('loadedmetadata', () => console.log('remoteAudio: loadedmetadata'));
    }
    async function startRtpStats(pc) {
      try {
        const sender = pc.getSenders().find(s => s.track && s.track.kind === 'audio');
        const receiver = pc.getReceivers().find(r => r.track && r.track.kind === 'audio');
        setInterval(async () => {
          if (sender) {
            const stats = await sender.getStats();
            for (const r of stats.values()) {
              if (r.type === 'outbound-rtp') console.log('RTP out bytesSent:', r.bytesSent);
            }
          }
          if (receiver) {
            const stats = await receiver.getStats();
            for (const r of stats.values()) {
              if (r.type === 'inbound-rtp') console.log('RTP in bytesReceived:', r.bytesReceived);
            }
          }
        }, 2000);
      } catch (e) { console.warn('stats error', e); }
    }
    // Send a text item then ask for a response (useful to verify DC + TTS path even if VAD fails)
    function sendTextAndRespond(text) {
      if (!dc || dc.readyState !== 'open') return;
      try {
        // New schema: item must be a "message" with role and content[]
        dc.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: [
              { type: 'input_text', text }
            ]
          }
        }));
        // Ask the model to produce a response (audio+text as per session voice)
        dc.send(JSON.stringify({
          type: 'response.create',
          response: { modalities: ['audio', 'text'] }
        }));
        console.log('Sent text+response.create');
      } catch (e) { console.warn('sendTextAndRespond failed', e); }
    }

    // ---- HTTP fallback: chat + TTS when realtime responds with server_error ----
    async function httpLLM(messages) {
      const r = await fetch(`${API_BASE}/chat/completions`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: "gpt-4o-mini",
          max_tokens: 220,
          temperature: 0.7,
          messages
        })
      });
      const j = await r.json().catch(() => ({}));
      if (!r.ok) throw new Error(j?.error?.message || `chat fail ${r.status}`);
      return j?.choices?.[0]?.message?.content || "";
    }

    async function httpTTS(text) {
      const r = await fetch(`${API_BASE}/audio/speech`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: "tts-1",
          voice: "marin",
          input: text,
          format: "mp3"
        })
      });
      if (!r.ok) {
        const t = await r.text();
        throw new Error(`tts fail ${r.status}: ${t.slice(0, 200)}`);
      }
      const blob = await r.blob();
      const url = URL.createObjectURL(blob);
      const a = document.getElementById('fallbackAudio');
      a.src = url;
      await a.play().catch(() => { });
      return url;
    }

    async function fallbackReplyFromHTTP() {
      try {
        statusEl && (statusEl.textContent = 'Fallback: thinking...');
        // Use rolling memory as context; last user message is already appended
        const msgs = buildMessages(/* userText unused; memory already contains it */);
        const text = (await httpLLM(msgs)).trim();
        if (text) {
          addMessage(text, 'elena');
          memory.messages.push({ role: 'assistant', content: text });
          saveMemory();
          await httpTTS(text);
        }
        statusEl && (statusEl.textContent = 'Idle');
      } catch (e) {
        console.error('HTTP fallback failed:', e);
        statusEl && (statusEl.textContent = 'Error');
      }
    }
    // ---- end HTTP fallback ----

    // --- Conversation memory (rolling window + running summary persisted to localStorage) ---
    const SYSTEM_PROMPT = "You are Elena, an empathetic supportive assistant. Be warm, validating, and concise. Default to 1–2 short sentences unless asked for detail. Avoid diagnoses and crisis guidance. Speak clearly and at a natural pace.";

    const MEMORY_KEY = "rizma_memory_v1";
    const MAX_TURNS_TO_SEND = 6; // send at most last 6 user+assistant exchanges (12 messages)

    let memory = {
      summary: "",                 // running summary string
      messages: []                 // full chronological list of {role:'user'|'assistant', content:string}
    };

    function saveMemory() {
      try { localStorage.setItem(MEMORY_KEY, JSON.stringify({ summary: memory.summary, messages: memory.messages })); } catch { }
    }

    function loadMemory() {
      try {
        const raw = localStorage.getItem(MEMORY_KEY);
        if (!raw) return;
        const data = JSON.parse(raw);
        memory.summary = data.summary || "";
        memory.messages = Array.isArray(data.messages) ? data.messages : [];
      } catch { }
    }

    function clearMemory() {
      memory.summary = "";
      memory.messages = [];
      saveMemory();
      if (chatEl) chatEl.innerHTML = "";
      hasSessionStarted = false;
      setBtnRecordingUI(false);
      if (statusEl) statusEl.textContent = "Idle";
      disconnectRealtime();
    }

    function renderHistory() {
      if (!chatEl || !memory.messages.length) return;
      chatEl.innerHTML = "";
      for (const m of memory.messages) {
        addMessage(m.content, m.role === 'user' ? 'user' : 'elena');
      }
    }

    // Build the message array we send to the model: system + summary + recent turns + new user text
    function buildMessages(userText) {
      const msgs = [{ role: "system", content: SYSTEM_PROMPT }];
      if (memory.summary) {
        msgs.push({ role: "system", content: "Conversation summary so far:\n" + memory.summary });
      }
      const recentTurns = memory.messages.slice(-MAX_TURNS_TO_SEND * 2); // user+assistant pairs
      // ecentTurns already contains the last msg; it was added as transcript in memory.messages.push
      msgs.push(...recentTurns);
      return msgs;
    }

    // Periodically compress older turns into the running summary
    async function maybeSummarize() {
      const LIMIT = MAX_TURNS_TO_SEND * 2;
      if (memory.messages.length <= LIMIT) return;

      // Everything except the last LIMIT messages gets summarized
      const toSummarize = memory.messages.slice(0, memory.messages.length - LIMIT);
      const contextText = toSummarize.map(m => `${m.role.toUpperCase()}: ${m.content}`).join("\n\n");

      const r = await fetch(`${API_BASE}/chat/completions`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: "gpt-4o-mini",
          messages: [
            { role: "system", content: "Summarize the conversation so far in 120-180 words. Capture facts, preferences, goals, and open questions. Output plain text only." },
            { role: "user", content: `Existing summary:\n${memory.summary || "(none)"}\n\nNew dialogue to fold in:\n${contextText}` }
          ]
        })
      });
      if (r.ok) {
        const j = await r.json();
        const newSummary = j.choices?.[0]?.message?.content || "";
        if (newSummary) {
          memory.summary = newSummary.trim();
          // Drop the summarized messages; keep only the last LIMIT
          memory.messages = memory.messages.slice(-LIMIT);
          saveMemory();
        }
      }
    }

    // Load from localStorage on startup and render any prior history
    loadMemory();
    renderHistory();
    // --- End conversation memory ---

    // --- Adaptive controls: desktop = click/keyboard toggle; mobile = press-and-hold ---
    function setBtnRecordingUI(rec) {
      btn.setAttribute('aria-pressed', rec ? 'true' : 'false');
      // Compute state label for screen readers and tooltip
      let label;
      if (!isConnected) label = 'Start Voice Session';
      else label = rec ? 'Stop Recording' : (hasSessionStarted ? 'Continue Recording' : 'Start Voice Session');
      btn.setAttribute('aria-label', label);
      btn.title = label;
      // Keep icons; just tint backgrounds
      btn.style.background = rec ? '#F7C2B0' : '#FBD5C7';
      if (micFab) {
        micFab.setAttribute('aria-label', label);
        micFab.title = label;
        // When recording, force peach; when idle, let CSS control default gray and hover
        if (rec) {
          micFab.style.background = '#F7C2B0';
        } else {
          micFab.style.background = '';
        }
      }
    }

    function bindControls() {
      btn.addEventListener('click', async (e) => {
        e.preventDefault();
        if (!isConnected) {
          if (isConnecting) return; // Prevent duplicate handshakes
          await connectRealtime();        // start session & mic
          setBtnRecordingUI(true);
        } else {
          // toggle mic on/off within the live session
          const track = micStream?.getAudioTracks?.()[0];
          const nowRec = track ? !track.enabled : false;
          if (track) track.enabled = nowRec;
          if (nowRec) hasSessionStarted = true;
          setBtnRecordingUI(nowRec);
          statusEl && (statusEl.textContent = nowRec ? 'Listening...' : 'Idle');
        }
      });
      if (micFab) {
        micFab.addEventListener('click', (e) => {
          e.preventDefault();
          btn.click(); // delegate to hidden primary handler
        });
      }
      // Accessibility: Space/Enter toggles
      document.addEventListener('keydown', (e) => {
        if (e.key === ' ' || e.key === 'Spacebar' || e.key === 'Enter') {
          e.preventDefault();
          btn.click();
        }
      });
      if (endBtn) {
        endBtn.addEventListener('click', (e) => {
          e.preventDefault();
          disconnectRealtime();
          if (panelEl) panelEl.style.display = 'none';
          if (composerEl) composerEl.style.display = 'flex';
        });
      }
    }
    bindControls();
    // --- End adaptive controls ---

    function addMessage(text, sender) {
      if (!chatEl) return;
      const row = document.createElement('div');
      row.style.display = 'flex';
      row.style.justifyContent = sender === 'user' ? 'flex-end' : 'flex-start';
      const bubble = document.createElement('div');
      bubble.textContent = (text || '').trim();
      bubble.style.maxWidth = '80%';
      bubble.style.padding = '10px 12px';
      bubble.style.border = 'none';
      bubble.style.borderRadius = '12px';
      bubble.style.whiteSpace = 'pre-wrap';
      bubble.style.wordBreak = 'break-word';
      // Colors: user on right = lighter peach, Elena on left = very light gray
      if (sender === 'user') {
        bubble.style.background = '#FFE6DE';   // lighter peach
      } else {
        bubble.style.background = '#F5F5F5';   // very light gray for Elena
      }
      row.appendChild(bubble);
      chatEl.appendChild(row);
      chatEl.scrollTop = chatEl.scrollHeight;
    }

    // --- Response triggering over DataChannel ---
    let responseRequested = false;
    function sendResponseCreate(extra = {}) {
      if (!dc || dc.readyState !== 'open') return;
      const payload = {
        type: 'response.create',
        response: {
          modalities: ['audio', 'text'],
          // voice is configured at session level via session.update
          ...extra
        }
      };
      try { dc.send(JSON.stringify(payload)); }
      catch (e) { console.warn('response.create send failed', e); }
    }
    // --- end response helpers ---

    // Wait for local ICE gathering to finish before sending SDP (no trickle), with timeout
    function waitForIceGatheringComplete(pc, timeoutMs = 3000) {
      if (pc.iceGatheringState === 'complete') return Promise.resolve('complete');
      return new Promise((resolve) => {
        const onChange = () => {
          if (pc.iceGatheringState === 'complete') {
            pc.removeEventListener('icegatheringstatechange', onChange);
            clearTimeout(t);
            resolve('complete');
          }
        };
        const t = setTimeout(() => {
          pc.removeEventListener('icegatheringstatechange', onChange);
          resolve('timeout'); // proceed with whatever we have
        }, timeoutMs);
        pc.addEventListener('icegatheringstatechange', onChange);
      });
    }
    // --- Realtime: WebRTC connection + event handling ---
    async function getEphemeralKey() {
      // Your Cloudflare Worker should create an ephemeral session token by POSTing to
      // https://api.openai.com/v1/realtime/sessions with your server-side API key.
      // It must return JSON that includes { client_secret: { value } }.
      const r = await fetch(SESSION_URL, { method: 'POST' });
      const ct = r.headers.get('content-type') || '';
      if (!r.ok) {
        const txt = await r.text();
        throw new Error(`Session POST failed ${r.status}. URL=${SESSION_URL}. Content-Type=${ct}. Body=${txt.slice(0, 500)}`);
      }
      if (!ct.includes('application/json')) {
        const txt = await r.text();
        throw new Error(`Session endpoint returned non-JSON. URL=${SESSION_URL}. Content-Type=${ct}. Body=${txt.slice(0, 500)}`);
      }
      const j = await r.json();
      if (j?.model) console.log('Realtime session model:', j.model);
      const key = j?.client_secret?.value || j?.client_secret?.secret || j?.client_secret;
      if (!key) throw new Error(`No ephemeral key in /session response: ${JSON.stringify(j).slice(0, 500)}`);
      return key;
    }

    async function connectRealtime() {
      if (isConnected || isConnecting) return;
      isConnecting = true;
      statusEl && (statusEl.textContent = 'Connecting...');
      btn.disabled = true;
      try {
        // 1) Get mic
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // 2) Peer connection and remote audio sink
        pc = new RTCPeerConnection({
          iceServers: [
            { urls: 'stun:stun.l.google.com:19302' },
            { urls: 'stun:stun1.l.google.com:19302' },
            { urls: 'stun:stun2.l.google.com:19302' },
            { urls: 'stun:stun3.l.google.com:19302' },
            { urls: 'stun:stun4.l.google.com:19302' }
          ],
          iceCandidatePoolSize: 1
        });
        // Start fetching the ephemeral key in parallel to ICE/mic to shave seconds
        const ephemeralPromise = getEphemeralKey();
        pc.addEventListener('iceconnectionstatechange', () => console.log('ICE:', pc.iceConnectionState));
        pc.addEventListener('connectionstatechange', () => console.log('PC:', pc.connectionState));
        pc.addEventListener('signalingstatechange', () => console.log('SIG:', pc.signalingState, 'ICEG:', pc.iceGatheringState));
        // ICE candidate logging (optional but useful)
        pc.onicecandidate = (ev) => {
          if (ev.candidate) console.log('ICE cand:', ev.candidate.type || ev.candidate.candidate);
        };
        const remoteAudio = document.getElementById('remoteAudio');
        pc.ontrack = (e) => {
          console.log('ontrack:', e.track.kind, 'streams:', e.streams?.length || 0);
          if (!remoteAudio) return;
          let ms = remoteAudio.srcObject;
          if (!(ms instanceof MediaStream)) {
            ms = new MediaStream();
            remoteAudio.srcObject = ms;
          }
          // Add the track if it's not already present
          const ids = ms.getTracks().map(t => t.id);
          if (!ids.includes(e.track.id)) {
            ms.addTrack(e.track);
          }
          // Ensure element is audible and playing
          remoteAudio.muted = false;
          remoteAudio.volume = 1.0;
          remoteAudio.controls = true; // turn on for debugging
          remoteAudio.play().catch(err => console.warn('remoteAudio.play failed:', err));
        };
        attachRemoteAudioDebug(remoteAudio);

        function wireDataChannel(channel) {
          if (!channel) return;
          // if we already have a live DC, ignore extras
          if (dc && dc.readyState !== 'closed' && dc.readyState !== 'closing') {
            try { channel.close(); } catch { }
            return;
          }
          dc = channel;
          dc.onmessage = async (e) => {
            try { await handleServerEvent(JSON.parse(e.data)); } catch (err) { console.warn('DC parse error', err, e.data); }
          };
          dc.onopen = () => {
            console.log('DC open');
            const update = {
              type: 'session.update',
              session: {
                instructions: SYSTEM_PROMPT,
                voice: 'marin',
                modalities: ['audio', 'text'],
                turn_detection: { type: 'server_vad' }
              }
            };
            dc.send(JSON.stringify(update));
            // DEBUG: force a hello so we know audio output is working
            setTimeout(() => sendTextAndRespond("Say only: hello, this is Elena."), 300);
          };
          dc.onclose = () => console.log('DC closed');
        }
        pc.ondatachannel = (e) => wireDataChannel(e.channel);

        // Ensure an audio m= section for send & recv
        pc.addTransceiver('audio', { direction: 'sendrecv' });

        // 3) Data channel for events (create one proactively; server may also open one)
        wireDataChannel(pc.createDataChannel('oai-events'));

        // 4) Add mic track (enabled initially)
        micStream.getAudioTracks().forEach(t => { t.enabled = true; pc.addTrack(t, micStream); });

        // 5) Create SDP offer
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);
        // Ensure ICE candidates are included in our SDP (no trickle)
        const iceResult = await waitForIceGatheringComplete(pc, 3000); // don't block forever
        console.log('ICE gathering result:', iceResult);
        const localSdp = pc.localDescription.sdp;

        // 6) Exchange SDP with OpenAI Realtime
        const EPHEMERAL = await ephemeralPromise;
        // Use the model bound to the ephemeral session (avoid client/server model mismatch)
        const url = "https://api.openai.com/v1/realtime?model=gpt-realtime";
        const sdpRes = await fetch(url, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${EPHEMERAL}`,
            'Content-Type': 'application/sdp'
          },
          body: localSdp
        });
        if (!sdpRes.ok) throw new Error(await sdpRes.text());
        const answer = await sdpRes.text();
        // Only set remote description when we're in the right signaling state
        if (pc.signalingState === 'have-local-offer') {
          await pc.setRemoteDescription({ type: 'answer', sdp: answer });
        } else {
          console.warn('Skipping setRemoteDescription; signalingState=', pc.signalingState);
        }
        // Debug: log receiver tracks after answer
        pc.getReceivers().forEach(r => {
          if (r.track) console.log('receiver track:', r.track.kind, r.track.readyState);
        });
        // Nudge remote audio to start (may need explicit play after user gesture)
        if (remoteAudio && remoteAudio.paused) {
          try { await remoteAudio.play(); } catch (e) { /* autoplay may be blocked until audio arrives */ }
        }
        startRtpStats(pc);

        isConnected = true;
        isRecording = true;
        hasSessionStarted = true;
        statusEl && (statusEl.textContent = 'Listening...');
        if (panelEl) panelEl.style.display = 'block';
        if (composerEl) composerEl.style.display = 'none';
        isConnecting = false;
        btn.disabled = false;
      } finally {
        isConnecting = false;
        btn.disabled = false;
      }
    }

    function disconnectRealtime() {
      try {
        pc && pc.close();
      } catch { }
      if (micStream) micStream.getTracks().forEach(t => t.stop());
      pc = null; dc = null; micStream = null;
      isConnected = false; isRecording = false;
      setBtnRecordingUI(false);
      statusEl && (statusEl.textContent = 'Idle');
      if (panelEl) panelEl.style.display = 'none';
      if (composerEl) composerEl.style.display = 'flex';
    }

    async function handleServerEvent(evt) {
      console.debug('RT event', evt.type, evt);
      // Common realtime events we care about:
      // - input_audio_buffer.speech_started / speech_stopped
      // - conversation.item.input_audio_transcription.completed (user transcript)
      // - response.audio_transcript.delta / .done (assistant transcript)
      // - response.done (assistant finalization; often includes full transcript)
      switch (evt.type) {
        case 'input_audio_buffer.speech_started':
          statusEl && (statusEl.textContent = 'Listening...');
          responseRequested = false; // new turn started
          break;
        case 'input_audio_buffer.speech_stopped':
          statusEl && (statusEl.textContent = 'Thinking...');
          if (!SERVER_VAD && !responseRequested) {
            sendResponseCreate();
            responseRequested = true;
          }
          break;
        case 'conversation.item.input_audio_transcription.completed': {
          const text = evt?.transcript || evt?.text || evt?.item?.input_audio_transcription?.text || '';
          if (text?.trim()) {
            addMessage(text.trim(), 'user');
            memory.messages.push({ role: 'user', content: text.trim() });
            saveMemory();
          }
          if (!SERVER_VAD && !responseRequested) {
            sendResponseCreate();
            responseRequested = true;
          }
          break;
        }
        case 'response.audio_transcript.delta': {
          const d = evt?.delta || '';
          if (d) assistantBuf += d;
          break;
        }
        case 'response.audio_transcript.done':
        case 'response.done': {
          // If server reports failure, fall back to HTTP pipeline
          /*
          const status = evt?.response?.status;
          if (status === 'failed') {
            console.error('Realtime response failed:', evt?.response?.status_details || evt);
            await fallbackReplyFromHTTP();
            const track = micStream?.getAudioTracks?.()[0];
            if (track) track.enabled = true;
            setBtnRecordingUI(true);
            break;
          }
          */
          // Success path: use transcript if present; else buffered deltas
          const explicit = evt?.transcript || evt?.response?.output_text || '';
          const finalText = (explicit && explicit.trim()) || assistantBuf.trim();
          if (finalText) {
            addMessage(finalText, 'elena');
            memory.messages.push({ role: 'assistant', content: finalText });
            saveMemory();
            assistantBuf = '';
          }
          statusEl && (statusEl.textContent = 'Idle');
          const track = micStream?.getAudioTracks?.()[0];
          if (track) track.enabled = true;
          setBtnRecordingUI(true);
          break;
        }
        case 'response.output_text.delta': {
          const d = evt?.delta || '';
          if (d) assistantBuf += d;
          break;
        }
        case 'response.output_text.done': {
          const finalText = (evt?.text || '').trim();
          if (finalText) {
            addMessage(finalText, 'elena');
            memory.messages.push({ role: 'assistant', content: finalText });
            saveMemory();
            assistantBuf = '';
          }
          statusEl && (statusEl.textContent = 'Idle');
          const track = micStream?.getAudioTracks?.()[0];
          if (track) track.enabled = true;
          setBtnRecordingUI(true);
          break;
        }
        case 'error': {
          console.error('Realtime error:', evt);
          statusEl && (statusEl.textContent = 'Error');
          break;
        }
        default:
          // Other events can be logged for debugging if needed
          console.debug('RT other', evt);
          break;
      }
    }
    // --- End Realtime: WebRTC connection + event handling ---

    // Reset Session clears memory and UI
    const resetBtn = document.getElementById('reset');
    if (resetBtn) {
      resetBtn.addEventListener('click', () => {
        clearMemory();
      });
    }
  </script>
</body>

</html>